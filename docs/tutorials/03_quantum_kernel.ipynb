{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Kernel Machine Learning\n",
    "\n",
    "## Overview\n",
    "\n",
    "The general task of machine learning is to find and study patterns in data. For many datasets, the datapoints are better understood in a higher dimensional feature space, which can be accessed through mapping functions known as *kernels*. This is the fundamental principle behind a series of machine learning algorithms known as *kernel methods*.\n",
    "\n",
    "In this notebook, you will learn how to define quantum kernels using `qiskit-machine-learning` and how these can be plugged into different algorithms to solve classification and clustering problems.\n",
    "\n",
    "The content is structured as follows:\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Classification](#class)    \n",
    "3. [Clustering](#clustering)\n",
    "4. [Conclusion](#conclusion)\n",
    "\n",
    "All examples used in this tutorial are based on this reference paper: [_Supervised learning with quantum enhanced feature spaces_](https://arxiv.org/pdf/1804.11326.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a> \n",
    "## 1. Introduction\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "*Support Vector Machines (SVMs)* are the best known example of kernel methods. They are supervised learning algorithms commonly used for classification tasks. The main goal of these SVMs is to find decision boundaries to separates a given set of data points into classes. Formally, the boundaries are hyperplanes in a high dimensional space, and this is where that kernels come into action. \n",
    "\n",
    "Simply put, a kernel function maps (transforms) input data from a low-dimensional space into a higher dimensional space. This transformation may allow data distributions that were originally not linearly separable to become a linearly separable problem. This effect is know as the \"kernel trick\".\n",
    "\n",
    "In general, the larger the distance of this boundary to datapoints from both classes is, the better the separation, so training algorithms aim to maximize this margin. \n",
    "\n",
    "### Kernel Functions\n",
    "\n",
    "Mathematically, kernel functions follow:\n",
    "\n",
    "$k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle$\n",
    "\n",
    "where \n",
    "* $k$ is the kernel function, \n",
    "* $\\vec{x}_i, \\vec{x}_j$ are $n$ dimensional inputs, \n",
    "* $f$ is a map from $n$-dimension to $m$-dimension space and \n",
    "* $\\langle a,b \\rangle$ denotes the dot product. \n",
    "\n",
    "When considering finite data, a kernel function can be represented as a matrix: \n",
    "\n",
    "$K_{ij} = k(\\vec{x}_i,\\vec{x}_j)$.\n",
    "\n",
    "### Quantum Kernels\n",
    "\n",
    "The main idea behind quantum kernel machine learning is to leverage quantum feature maps to perform the kernel trick. In this case, the quantum kernel is created by mapping a classical feature vector $\\vec{x}$ to a Hilbert space using a quantum feature map $\\phi(\\vec{x})$. Mathematically:\n",
    "\n",
    "$K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}$\n",
    "\n",
    "where \n",
    "* $K_{ij}$ is the kernel matrix, \n",
    "* $\\vec{x}_i, \\vec{x}_j$ are $n$ dimensional inputs, \n",
    "* $\\phi(\\vec{x})$ is the quantum feature map\n",
    "* $\\langle a,b \\rangle$ denotes the overlap of two quantum states $a$ and $b$. \n",
    "\n",
    "Once the quantum kernel is defined, it can be trained using classical training algorithms such as SVMs or clustering algorithms, as you will see in the examples below.\n",
    "\n",
    "Before introducing any example, we set up the global seed to ensure reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.utils import algorithm_globals\n",
    "\n",
    "algorithm_globals.random_seed = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='class'></a> \n",
    "## 2. Classification\n",
    "\n",
    "This section illustrates a quantum kernel classification workflow using `qiskit-machine-learning`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Defining the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use the _ad hoc dataset_ as described in the reference [paper](https://arxiv.org/pdf/1804.11326.pdf). \n",
    "\n",
    "We can define the dataset dimension and get our train and test subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'qiskit_machine_learning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-54c6ab19ad0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mqiskit_machine_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mad_hoc_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0madhoc_dimension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m train_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n\u001b[1;32m      5\u001b[0m     \u001b[0mtraining_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'qiskit_machine_learning'"
     ]
    }
   ],
   "source": [
    "from qiskit_machine_learning.datasets import ad_hoc_data\n",
    "\n",
    "adhoc_dimension = 2\n",
    "train_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n",
    "    training_size=20,\n",
    "    test_size=5,\n",
    "    n=adhoc_dimension,\n",
    "    gap=0.3,\n",
    "    plot_data=False,\n",
    "    one_hot=False,\n",
    "    include_sample_total=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is 2-dimensional, the 2 features are represented by the x and y coordinates, and it has 2 class labels: A and B. We can plot it and see how the distribution looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.ylim(0, 2 * np.pi)\n",
    "plt.xlim(0, 2 * np.pi)\n",
    "plt.imshow(\n",
    "    np.asmatrix(adhoc_total).T,\n",
    "    interpolation=\"nearest\",\n",
    "    origin=\"lower\",\n",
    "    cmap=\"RdBu\",\n",
    "    extent=[0, 2 * np.pi, 0, 2 * np.pi],\n",
    ")\n",
    "\n",
    "# A train plot\n",
    "plt.scatter(\n",
    "    train_features[\n",
    "        np.where(train_labels[:] == 0), 0\n",
    "    ],  # x coordinate of train_labels where class is 0\n",
    "    train_features[\n",
    "        np.where(train_labels[:] == 0), 1\n",
    "    ],  # y coordinate of train_labels where class is 0\n",
    "    marker=\"s\",\n",
    "    facecolors=\"w\",\n",
    "    edgecolors=\"b\",\n",
    "    label=\"A train\",\n",
    ")\n",
    "\n",
    "# B train plot\n",
    "plt.scatter(\n",
    "    train_features[\n",
    "        np.where(train_labels[:] == 1), 0\n",
    "    ],  # x coordinate of train_labels where class is 1\n",
    "    train_features[\n",
    "        np.where(train_labels[:] == 1), 1\n",
    "    ],  # y coordinate of train_labels where class is 1\n",
    "    marker=\"o\",\n",
    "    facecolors=\"w\",\n",
    "    edgecolors=\"r\",\n",
    "    label=\"B train\",\n",
    ")\n",
    "\n",
    "# A test plot\n",
    "plt.scatter(\n",
    "    test_features[np.where(test_labels[:] == 0), 0],  # x coordinate of test_labels where class is 0\n",
    "    test_features[np.where(test_labels[:] == 0), 1],  # y coordinate of test_labels where class is 0\n",
    "    marker=\"s\",\n",
    "    facecolors=\"b\",\n",
    "    edgecolors=\"w\",\n",
    "    label=\"A test\",\n",
    ")\n",
    "\n",
    "# B test plot\n",
    "plt.scatter(\n",
    "    test_features[np.where(test_labels[:] == 1), 0],  # x coordinate of test_labels where class is 1\n",
    "    test_features[np.where(test_labels[:] == 1), 1],  # y coordinate of test_labels where class is 1\n",
    "    marker=\"o\",\n",
    "    facecolors=\"r\",\n",
    "    edgecolors=\"w\",\n",
    "    label=\"B test\",\n",
    ")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "plt.title(\"Ad hoc dataset for classification\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Defining the quantum kernel\n",
    "\n",
    "The next step is to create a quantum kernel instance that will help classify this data. \n",
    "\n",
    "We use the `FidelityQuantumKernel` [class](https://github.com/Qiskit/qiskit-machine-learning/blob/main/qiskit_machine_learning/kernels/fidelity_quantum_kernel.py), and pass two input arguments to its constructor: \n",
    "\n",
    "1. `feature_map`: in this case, a 2-qubit `ZZFeatureMap` [instance](https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html).\n",
    "\n",
    "2. `fidelity`: in this case, the `ComputeUncompute` [fidelity](https://qiskit.org/documentation/stubs/qiskit.algorithms.state_fidelities.ComputeUncompute.html) subroutine that leverages the `Sampler` [primitive](https://qiskit.org/documentation/stubs/qiskit.primitives.Sampler.html).\n",
    "\n",
    "**NOTE:** If you don't pass a `Sampler` or `Fidelity` instance, instances of the reference `Sampler` and `ComputeUncompute` classes (found in `qiskit.primitives`) will be created by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit.algorithms.state_fidelities import ComputeUncompute\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "\n",
    "adhoc_feature_map = ZZFeatureMap(feature_dimension=adhoc_dimension, reps=2, entanglement=\"linear\")\n",
    "\n",
    "sampler = Sampler()\n",
    "\n",
    "fidelity = ComputeUncompute(sampler=sampler)\n",
    "\n",
    "adhoc_kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=adhoc_feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Classification with SVC\n",
    "The quantum kernel can now be plugged into classical kernel methods, such as the `SVC` [algorithm](https://scikit-learn.org/stable/modules/svm.html) from `scikit-learn`. This algorithm allows us to define a [custom kernel](https://scikit-learn.org/stable/modules/svm.html#custom-kernels) in two ways: \n",
    "1. by providing the kernel as a **callable function**\n",
    "2. by precomputing the **kernel matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Kernel as a callable function\n",
    "\n",
    "We define a SVC model and directly pass the `evaluate` function of the quantum kernel as a callable. Once the model is created, we train it by calling the `fit` method on the training dataset and evaluate the model for accuracy with `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "adhoc_svc = SVC(kernel=adhoc_kernel.evaluate)\n",
    "\n",
    "adhoc_svc.fit(train_features, train_labels)\n",
    "\n",
    "adhoc_score_callable_function = adhoc_svc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Callable kernel classification test score: {adhoc_score_callable_function}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Precomputed kernel matrix\n",
    "\n",
    "Instead of passing a function of the quantum kernel as a callable, we can also precompute training and testing kernel matrices before passing them to the `scikit-learn` `SVC` algorithm. \n",
    "\n",
    "To extract the train and test matrices, we can call `evaluate` on the previoulsy defined kernel as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adhoc_matrix_train = adhoc_kernel.evaluate(x_vec=train_features)\n",
    "adhoc_matrix_test = adhoc_kernel.evaluate(x_vec=test_features, y_vec=train_features)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(\n",
    "    np.asmatrix(adhoc_matrix_train), interpolation=\"nearest\", origin=\"upper\", cmap=\"Blues\"\n",
    ")\n",
    "axs[0].set_title(\"Ad hoc training kernel matrix\")\n",
    "\n",
    "axs[1].imshow(np.asmatrix(adhoc_matrix_test), interpolation=\"nearest\", origin=\"upper\", cmap=\"Reds\")\n",
    "axs[1].set_title(\"Ad hoc testing kernel matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use these matrices, we set the `kernel` parameter of a new `SVC` instance to `\"precomputed\"`. We train the classifier by calling `fit` with the training matrix and training dataset. Once the model is trained, we evaluate it using the test matrix on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhoc_svc = SVC(kernel=\"precomputed\")\n",
    "\n",
    "adhoc_svc.fit(adhoc_matrix_train, train_labels)\n",
    "\n",
    "adhoc_score_precomputed_kernel = adhoc_svc.score(adhoc_matrix_test, test_labels)\n",
    "\n",
    "print(f\"Precomputed kernel classification test score: {adhoc_score_precomputed_kernel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Classification with QSVC\n",
    "\n",
    "`QSVC` is an alternative training algorithm provided by `qiskit-machine-learning` for convenience. It is an extension of `SVC` that takes in a quantum kernel instad of the `kernel.evaluate` method shown before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "\n",
    "qsvc = QSVC(quantum_kernel=adhoc_kernel)\n",
    "\n",
    "qsvc.fit(train_features, train_labels)\n",
    "\n",
    "qsvc_score = qsvc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"QSVC classification test score: {qsvc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Evaluation of models used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classification Model                    | Accuracy Score\")\n",
    "print(f\"---------------------------------------------------------\")\n",
    "print(f\"SVC using kernel as a callable function | {adhoc_score_callable_function:10.2f}\")\n",
    "print(f\"SVC using precomputed kernel matrix     | {adhoc_score_precomputed_kernel:10.2f}\")\n",
    "print(f\"QSVC                                    | {qsvc_score:10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the classifcation dataset is small, we find that the 3 models achieve 100% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering'></a> \n",
    "## 3. Clustering\n",
    "\n",
    "The second workflow in this tutorial focuses on a clustering task using `qiskit-machine-learning` and the spectral clustering algorithm from `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Defining the dataset\n",
    "\n",
    "We will once again use the _ad hoc dataset_, but now generated with a gap of `0.6` (previous example: `0.3`) between the two classes. \n",
    "\n",
    "Note that clustering falls under the category of unsupervised machine learning, so a test sample is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhoc_dimension = 2\n",
    "train_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n",
    "    training_size=25,\n",
    "    test_size=0,\n",
    "    n=adhoc_dimension,\n",
    "    gap=0.6,\n",
    "    plot_data=False,\n",
    "    one_hot=False,\n",
    "    include_sample_total=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We plot the clustering dataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.ylim(0, 2 * np.pi)\n",
    "plt.xlim(0, 2 * np.pi)\n",
    "plt.imshow(\n",
    "    np.asmatrix(adhoc_total).T,\n",
    "    interpolation=\"nearest\",\n",
    "    origin=\"lower\",\n",
    "    cmap=\"RdBu\",\n",
    "    extent=[0, 2 * np.pi, 0, 2 * np.pi],\n",
    ")\n",
    "\n",
    "# A label plot\n",
    "plt.scatter(\n",
    "    # x coordinate (index 0) of training data where class is A or label is 0\n",
    "    train_features[np.where(train_labels[:] == 0), 0],\n",
    "    # y coordinate (index 1) of training data where class is A or label is 0\n",
    "    train_features[np.where(train_labels[:] == 0), 1],\n",
    "    marker=\"s\",\n",
    "    facecolors=\"w\",\n",
    "    edgecolors=\"b\",\n",
    "    label=\"A\",\n",
    ")\n",
    "\n",
    "# B label plot\n",
    "plt.scatter(\n",
    "    # x coordinate (index 0) of training data where class is B or label is 1\n",
    "    train_features[np.where(train_labels[:] == 1), 0],\n",
    "    # y coordinate (index 1) of training data where class is B or label is 1\n",
    "    train_features[np.where(train_labels[:] == 1), 1],\n",
    "    marker=\"o\",\n",
    "    facecolors=\"w\",\n",
    "    edgecolors=\"r\",\n",
    "    label=\"B\",\n",
    ")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "plt.title(\"Ad hoc dataset for clustering\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Defining the Quantum Kernel\n",
    "We use an identical setup as in the classification example. We create another instance of the `FidelityQuantumKernel` class with a `ZZFeatureMap`, but you might notice that in this case we do not provide a `fidelity` instance. This is because the `ComputeUncompute` method provided in the previous case is instantiated by default when the fidelity instance is not provided explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhoc_feature_map = ZZFeatureMap(feature_dimension=adhoc_dimension, reps=2, entanglement=\"linear\")\n",
    "\n",
    "adhoc_kernel = FidelityQuantumKernel(feature_map=adhoc_feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Clustering with the Spectral Clustering Model\n",
    "\n",
    "The `scikit-learn` spectral clustering algorithm allows us to define a custom kernel in two ways (just like `SVC`):\n",
    "1. by providing the kernel as a callable function\n",
    "2. by precomputing the kernel matrix. \n",
    "\n",
    "With the current `FidelityQuantumKernel` class in `qiskit-machine-learning`, we can only use the latter option, so we compute the kernel matrix by calling `evaluate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "adhoc_matrix = adhoc_kernel.evaluate(x_vec=train_features)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(np.asmatrix(adhoc_matrix), interpolation=\"nearest\", origin=\"upper\", cmap=\"Greens\")\n",
    "plt.title(\"Ad hoc clustering kernel matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a spectral clustering model and fit it using the precomputed kernel. Further, we score the labels using normalized mutual information, since we a priori know the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhoc_spectral = SpectralClustering(2, affinity=\"precomputed\")\n",
    "\n",
    "cluster_labels = adhoc_spectral.fit_predict(adhoc_matrix)\n",
    "\n",
    "cluster_score = normalized_mutual_info_score(cluster_labels, train_labels)\n",
    "\n",
    "print(f\"Clustering score: {cluster_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a> \n",
    "## 4. Conclusion\n",
    "\n",
    "In this tutorial:\n",
    "\n",
    "* We reviewed the fundamentals of quantum kernel learning\n",
    "* We understood how to define quantum kernels as instances of `FidelityQuantumKernel`\n",
    "* We learned how to use the `scikit-lean` `SVC` algorithm with a custom quantum kernel as a callable function\n",
    "* We learned how to use the `scikit-lean` `SVC` and `SpectralClustering` algorithms with a pre-computed quantum kernel matrix\n",
    "* We learned how to train classifiers with the `QSVC` algorithm from `qiskit-machine-learning` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further reference, `scikit-learn` has other algorithms that can use a precomputed kernel matrix, such as:\n",
    "\n",
    "- [Agglomerative clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
    "- [Support vector regression](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "- [Ridge regression](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html)\n",
    "- [Gaussian process regression](https://scikit-learn.org/stable/modules/gaussian_process.html)\n",
    "- [Principal component analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qiskit.tools.jupyter\n",
    "\n",
    "%qiskit_version_table\n",
    "%qiskit_copyright"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
